{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Keras_Bert and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/hpc/applications/anaconda/3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/hpc/applications/anaconda/3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/hpc/applications/anaconda/3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/hpc/applications/anaconda/3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/hpc/applications/anaconda/3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/hpc/applications/anaconda/3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras_bert import Tokenizer, load_vocabulary\n",
    "import os\n",
    "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n",
    "token_dict = load_vocabulary(vocab_path)\n",
    "tokenizer = Tokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "SEQ_LEN=64\n",
    "model = load_trained_model_from_checkpoint(config_path, checkpoint_path,\n",
    "                                          training = True,\n",
    "                                          trainable = True,\n",
    "                                          seq_len=SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use given case as testing data and use the rest three cases as training data. (Generalize Performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "SEQ_LENGTH = 64\n",
    "def get_data(case_name):\n",
    "    header_list = [\"file_name\", \"ignore\",\"persuasion\",\"speaker\",\"argument\"]\n",
    "    directoryPath = './../scratch/data/processed_csv/'\n",
    "    glued_data = pd.DataFrame()\n",
    "    test_data = pd.DataFrame()\n",
    "    for file_name in glob.glob(directoryPath+'*.csv'):\n",
    "        if file_name[32]!=case_name:\n",
    "            x = pd.read_csv(file_name, low_memory=False,names=header_list)\n",
    "            glued_data = pd.concat([glued_data,x],axis=0)\n",
    "        else:\n",
    "            x = pd.read_csv(file_name, low_memory=False,names=header_list)\n",
    "            test_data = pd.concat([test_data,x],axis=0)\n",
    "    return glued_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_name = 'R'\n",
    "train_data, test_data = get_data(case_name)\n",
    "train_arg = train_data['argument'].to_numpy()\n",
    "test_arg = test_data['argument'].to_numpy()\n",
    "train_y = train_data['persuasion']!=0\n",
    "test_y = test_data['persuasion']!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = []\n",
    "train_X1 = []\n",
    "test_X = []\n",
    "test_X1=[]\n",
    "for i in range(0,len(train_arg)):\n",
    "    ids, segments = tokenizer.encode(str(train_arg[i]),max_len=SEQ_LENGTH)\n",
    "    train_X.append(ids)\n",
    "    train_X1.append(segments)\n",
    "train_X = np.array(train_X)\n",
    "train_X1 = np.array(train_X1)\n",
    "for i in range(0,len(test_arg)):\n",
    "    ids, segments = tokenizer.encode(str(test_arg[i]),max_len=SEQ_LENGTH)\n",
    "    test_X.append(ids)\n",
    "    test_X1.append(segments)\n",
    "test_X = np.array(test_X)\n",
    "test_X1 = np.array(test_X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'Variable_6:0' shape=() dtype=int32> tp\n",
      "tracking <tf.Variable 'Variable_7:0' shape=() dtype=int32> fn\n"
     ]
    }
   ],
   "source": [
    "import keras_metrics\n",
    "from keras_radam import RAdam\n",
    "\n",
    "inputs = model.inputs[:2]\n",
    "dense = model.get_layer('NSP-Dense').output\n",
    "leave_one_out_outputs = keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "leave_one_out_model_R=keras.models.Model(inputs,leave_one_out_outputs)\n",
    "leave_one_out_model_R.compile(\n",
    "    RAdam(learning_rate = 1e-4),\n",
    "    loss='binary_crossentropy',metrics=[keras_metrics.recall()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13608 samples, validate on 5249 samples\n",
      "Epoch 1/3\n",
      "13608/13608 [==============================] - 1267s 93ms/step - loss: 0.3070 - recall: 0.2153 - val_loss: 0.3783 - val_recall: 0.1053\n",
      "Epoch 2/3\n",
      "13608/13608 [==============================] - 1259s 93ms/step - loss: 0.2023 - recall: 0.3699 - val_loss: 0.3590 - val_recall: 0.1574\n",
      "Epoch 3/3\n",
      "13608/13608 [==============================] - 1261s 93ms/step - loss: 0.1400 - recall: 0.6022 - val_loss: 0.3356 - val_recall: 0.5412\n"
     ]
    }
   ],
   "source": [
    "leave_one_out_history_R = leave_one_out_model_R.fit([train_X,train_X1],train_y,epochs=3,batch_size=100,\n",
    "                               validation_data=([test_X,test_X1],test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More epochs might increase the performance. Still need to fine tune the model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "leave_one_out_y_pred_R = leave_one_out_model_R.predict([test_X,test_X1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4226  399]\n",
      " [ 236  388]]\n",
      "recall score: 0.6217948717948718\n",
      "precision: 0.49301143583227447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "leave_one_out_filtered = []\n",
    "for pred in leave_one_out_y_pred_R:\n",
    "    if pred>=0.5:\n",
    "        leave_one_out_filtered.append(1)\n",
    "    else:\n",
    "        leave_one_out_filtered.append(0)\n",
    "cm = confusion_matrix(leave_one_out_filtered,test_y)\n",
    "print(cm)\n",
    "tp,fp,fn,tn=cm.ravel()\n",
    "print('recall score:',tn/(fn+tn))\n",
    "print('precision:',(tn/(fp+tn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_name = 'T'\n",
    "train_data, test_data = get_data(case_name)\n",
    "train_arg = train_data['argument'].to_numpy()\n",
    "test_arg = test_data['argument'].to_numpy()\n",
    "train_y = train_data['persuasion']!=0\n",
    "test_y = test_data['persuasion']!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = []\n",
    "train_X1 = []\n",
    "test_X = []\n",
    "test_X1=[]\n",
    "for i in range(0,len(train_arg)):\n",
    "    ids, segments = tokenizer.encode(str(train_arg[i]),max_len=SEQ_LENGTH)\n",
    "    train_X.append(ids)\n",
    "    train_X1.append(segments)\n",
    "train_X = np.array(train_X)\n",
    "train_X1 = np.array(train_X1)\n",
    "for i in range(0,len(test_arg)):\n",
    "    ids, segments = tokenizer.encode(str(test_arg[i]),max_len=SEQ_LENGTH)\n",
    "    test_X.append(ids)\n",
    "    test_X1.append(segments)\n",
    "test_X = np.array(test_X)\n",
    "test_X1 = np.array(test_X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'Variable_8:0' shape=() dtype=int32> tp\n",
      "tracking <tf.Variable 'Variable_9:0' shape=() dtype=int32> fn\n"
     ]
    }
   ],
   "source": [
    "model = load_trained_model_from_checkpoint(config_path, checkpoint_path,\n",
    "                                          training = True,\n",
    "                                          trainable = True,\n",
    "                                          seq_len=SEQ_LEN)\n",
    "inputs = model.inputs[:2]\n",
    "dense = model.get_layer('NSP-Dense').output\n",
    "leave_one_out_outputs = keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "leave_one_out_model_T=keras.models.Model(inputs,leave_one_out_outputs)\n",
    "leave_one_out_model_T.compile(\n",
    "    RAdam(learning_rate = 1e-4),\n",
    "    loss='binary_crossentropy',metrics=[keras_metrics.recall()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11944 samples, validate on 6913 samples\n",
      "Epoch 1/3\n",
      "11944/11944 [==============================] - 1189s 100ms/step - loss: 0.3942 - recall: 0.3209 - val_loss: 0.2691 - val_recall: 0.3629\n",
      "Epoch 2/3\n",
      "11944/11944 [==============================] - 1171s 98ms/step - loss: 0.2082 - recall: 0.3998 - val_loss: 0.2717 - val_recall: 0.3217\n",
      "Epoch 3/3\n",
      "11944/11944 [==============================] - 1171s 98ms/step - loss: 0.1432 - recall: 0.6584 - val_loss: 0.2800 - val_recall: 0.5187\n"
     ]
    }
   ],
   "source": [
    "leave_one_out_history_T = leave_one_out_model_T.fit([train_X,train_X1],train_y,epochs=3,batch_size=100,\n",
    "                               validation_data=([test_X,test_X1],test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "leave_one_out_y_pred_T = leave_one_out_model_T.predict([test_X,test_X1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5671  448]\n",
      " [ 357  437]]\n",
      "recall score: 0.5503778337531486\n",
      "precision: 0.4937853107344633\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "leave_one_out_filtered = []\n",
    "for pred in leave_one_out_y_pred_T:\n",
    "    if pred>=0.5:\n",
    "        leave_one_out_filtered.append(1)\n",
    "    else:\n",
    "        leave_one_out_filtered.append(0)\n",
    "cm = confusion_matrix(leave_one_out_filtered,test_y)\n",
    "print(cm)\n",
    "tp,fp,fn,tn=cm.ravel()\n",
    "print('recall score:',tn/(fn+tn))\n",
    "print('precision:',(tn/(fp+tn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5205479452054795"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * ((tn/(fp+tn)) * (tn/(fn+tn))) / ((tn/(fp+tn)) + (tn/(fn+tn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two more models needed to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
